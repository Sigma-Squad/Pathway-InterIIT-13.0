{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements\n"
      ],
      "metadata": {
        "id": "3hY5lQ5RsySL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# web-rag\n",
        "!pip install -q mistralai\n",
        "!pip install -q requests\n",
        "!pip install -q beautifulsoup4\n",
        "!pip install -q selenium\n",
        "!pip install -q praw\n",
        "!pip install -q python-docx\n",
        "!pip install -q faiss-cpu -q\n",
        "\n",
        "## retrieval pipeline\n",
        "!pip install -q pathway\n",
        "!pip install -q llama-index\n",
        "!pip install -q llama-index-retrievers-pathway\n",
        "!pip install -q llama-index-embeddings-huggingface\n",
        "!pip install -q llama-index-embeddings-instructor\n",
        "\n",
        "## evidence graph\n",
        "!pip install -q llama-index-core\n",
        "!pip install -q llama-index-postprocessor-flag-embedding-reranker\n",
        "!pip install -q llama-index-graph-stores-neo4j\n",
        "!pip install -q llama-parse\n",
        "!pip install -q pyvis Ipython"
      ],
      "metadata": {
        "id": "IiXOvMAtQO4g",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n",
        "\n"
      ],
      "metadata": {
        "id": "d3qu6Bw5s5ok"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FsabNjlZMxC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import List, Dict, Tuple, Any\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from mistralai import Mistral\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.parse\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from docx import Document\n",
        "from textblob import TextBlob\n",
        "from datetime import datetime, timedelta\n",
        "import faiss\n",
        "import numpy as np\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "from typing import Dict, List, Tuple\n",
        "import os\n",
        "import nest_asyncio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization\n",
        "  - Mistral Client : model - \"mistral-large-latest\"\n",
        "  - Embedding Model : \"BAAI/bge-small-en-v1.5\"\n",
        "  - MistralLLM() : Custom Wrapper Object for the LlamaIndex Framework"
      ],
      "metadata": {
        "id": "Rsx-x3OHsjG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "# model_kwargs = {\"device\": \"cuda\"}\n",
        "embed_model = HuggingFaceEmbedding(\n",
        "    model_name=\"BAAI/bge-small-en-v1.5\",\n",
        "    # model_kwargs=model_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "4eQ6krSYQUwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "api_key = getpass(\"Enter the api key in the box beside : \") # API_KEY - oN2G2UkwPDgjeknYrr1oL3oPOWa7AETe\n",
        "client = Mistral(api_key=api_key)"
      ],
      "metadata": {
        "id": "m0uX4kYYsHU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Mistral API Caller\n",
        "\n",
        "def mistral_response(prompt: str) -> str:\n",
        "  model = \"mistral-large-latest\"\n",
        "  chat_response = client.chat.complete(\n",
        "      model= model,\n",
        "      temperature=0.5, # changed from 0.7 -> 0.5\n",
        "      messages = [\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt,\n",
        "          },\n",
        "      ]\n",
        "  )\n",
        "  print(chat_response.choices[0].message.content)\n",
        "  return chat_response.choices[0].message.content.strip()"
      ],
      "metadata": {
        "id": "8WnUyW6SvBpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MistralLLM: A custom LLM wrapper for integrating the Mistral model with the LlamaIndex framework.\n",
        "\n",
        "from llama_index.core.llms import (\n",
        "    CustomLLM,\n",
        "    CompletionResponse,\n",
        "    CompletionResponseGen,\n",
        "    LLMMetadata,\n",
        ")\n",
        "from llama_index.core.llms.callbacks import llm_completion_callback\n",
        "from llama_index.core import Settings\n",
        "import time\n",
        "\n",
        "class MistralLLM(CustomLLM):\n",
        "    context_window: int = 3900\n",
        "    num_output: int = 256\n",
        "    model_name: str = \"custom\"\n",
        "\n",
        "    @property\n",
        "    def metadata(self) -> LLMMetadata:\n",
        "        return LLMMetadata(\n",
        "            context_window=self.context_window,\n",
        "            num_output=self.num_output,\n",
        "            model_name=self.model_name,\n",
        "        )\n",
        "\n",
        "    @llm_completion_callback()\n",
        "    def complete(self, prompt: str, **kwargs) -> CompletionResponse:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        chat_response = client.chat.complete(\n",
        "            model=\"mistral-large-latest\",\n",
        "            messages=messages\n",
        "        )\n",
        "        response_text = chat_response.choices[0].message.content\n",
        "        time.sleep(1)\n",
        "        return CompletionResponse(text=response_text)\n",
        "\n",
        "    @llm_completion_callback()\n",
        "    def stream_complete(self, prompt: str, **kwargs) -> CompletionResponseGen:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        chat_response = client.chat.stream_complete(\n",
        "            model=\"mistral-large-latest\",\n",
        "            messages=messages\n",
        "        )\n",
        "        response = \"\"\n",
        "        for token in chat_response:\n",
        "            response += token\n",
        "            yield CompletionResponse(text=response, delta=token)\n"
      ],
      "metadata": {
        "id": "rlS5OoUBtQBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Setting the primary llm object to MistralLLM() and embed model to BAAI/bge-small-en-v1.5\n",
        "Settings.llm = MistralLLM()\n",
        "Settings.embed_model = embed_model"
      ],
      "metadata": {
        "id": "sv-XDJ78tVQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYN_ajufvs_J"
      },
      "source": [
        "# Chain of Thought"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlMOxYqhumkD"
      },
      "outputs": [],
      "source": [
        "def cot_tasks(method, prompt):\n",
        "\n",
        "  system_prompt = f\"\"\"\n",
        "  System Prompts:\n",
        "  Q:= Compare the privacy policy of 2 companies Company A and Company B.\n",
        "  A:= 1) Fetch the privacy policy of Company A.\n",
        "      2) Fetch the privacy policy of Company B.\n",
        "      3) Compare the privacy policy of Company A and Company B.\n",
        "  Q:= Give the Protection of Information from the privacy policy of Company A.\n",
        "  A:= 1) Fetch the privacy policy of Company A.\n",
        "      2) Give the Protection of Information from the privacy policy of Company A.\n",
        "  Q:= List down the Information Required to Detect Violations in the privacy policy of Company A.\n",
        "  A:= 1) Fetch the privacy policy of Company A.\n",
        "      2) List down the Information Required to Detect Violations in the privacy policy of Company A.\n",
        "  Just give the points and do not include question prompt and A:= tag.\n",
        "  \"\"\"\n",
        "  initial_prompt = f\"\"\"Main Task: {prompt}\\nBreak this task into a list of smaller subtasks. Give only the names of the smaller subtasks and not any description of that subtask. Just give the subtasks in as least number of points as possible. Give the answer following the pattern given in System Prompts.\\n\"\"\"\n",
        "\n",
        "  thought_process = system_prompt + \"\\n\" + initial_prompt\n",
        "\n",
        "  final_answer = \"\"\n",
        "  max_steps = 5\n",
        "  wait_time = 1\n",
        "  list_length = 0\n",
        "\n",
        "  modified_prompt = thought_process\n",
        "  modified_thought = mistral_response(modified_prompt)\n",
        "  subtasks = modified_thought.strip().split(\"\\n\")\n",
        "  subtasks = [subtask.strip().lower() for subtask in subtasks if subtask.strip()]\n",
        "  list_length = len(subtasks)\n",
        "\n",
        "  for step in range(max_steps):\n",
        "    modified_prompt = thought_process\n",
        "    try:\n",
        "      modified_thought = mistral_response(modified_prompt)\n",
        "    except:\n",
        "      time.sleep(wait_time)  # Wait for wait_time seconds before retrying\n",
        "      modified_thought = mistral_response(modified_prompt)\n",
        "    temp_list = modified_thought.strip().split(\"\\n\")\n",
        "    temp_list = [t_list.strip().lower() for t_list in temp_list if t_list.strip()]\n",
        "    temp_list_length = len(temp_list)\n",
        "    if temp_list_length < list_length:\n",
        "      subtasks = temp_list\n",
        "      list_length = temp_list_length\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  return subtasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKJmdY9dvwY4"
      },
      "source": [
        "# Search Query Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IZ6PwS7vkDh"
      },
      "outputs": [],
      "source": [
        "google_search_api = \"AIzaSyBYJYj-NVuHvdnNfOsI0xTlSdpu-1bL9uM\"\n",
        "cx = \"462d7268f48434ffa\"\n",
        "## Google Search\n",
        "def google_search(query, k=1):\n",
        "    try:\n",
        "        response = requests.get(\"https://www.googleapis.com/customsearch/v1\", params={\n",
        "            \"q\": query,\n",
        "            \"key\": google_search_api,\n",
        "            \"cx\": cx\n",
        "        })\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            raise ConnectionError(f\"Google Search API failed with status code: {response.status_code}\")\n",
        "\n",
        "        result_summary = []\n",
        "        data = response.json()\n",
        "        for i in range(k):\n",
        "            if \"items\" in data and data[\"items\"]:\n",
        "                result = data[\"items\"][i]\n",
        "                result_summary.append ({\n",
        "                    \"title\": result.get(\"title\"),\n",
        "                    \"link\": result.get(\"link\")\n",
        "                })\n",
        "\n",
        "        return result_summary\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        raise ConnectionError(f\"Google Search API request failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTd46JSov8g9"
      },
      "outputs": [],
      "source": [
        "bing_search_api = ''\n",
        "\n",
        "## Bing Search\n",
        "def bing_search(query, k=1):\n",
        "    try:\n",
        "        headers = {\"Ocp-Apim-Subscription-Key\": bing_search_api}\n",
        "        params = {\"q\": query, \"textDecorations\": True, \"textFormat\": \"HTML\"}\n",
        "\n",
        "        response = requests.get(\"https://api.bing.microsoft.com/v7.0/search\", headers=headers, params=params)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            raise ConnectionError(f\"Bing Search API failed with status code: {response.status_code}\")\n",
        "\n",
        "        data = response.json()\n",
        "        result_summary = []\n",
        "\n",
        "        for i in range(k):\n",
        "            if \"webPages\" in data and \"value\" in data[\"webPages\"] and data[\"webPages\"][\"value\"]:\n",
        "                best_result = data[\"webPages\"][\"value\"][i]\n",
        "                link = best_result.get(\"url\")\n",
        "                result_summary.append ({\n",
        "                    \"title\": best_result.get(\"name\"),\n",
        "                    \"link\": link\n",
        "                    })\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        raise ConnectionError(f\"Bing Search API request failed: {e}\")\n",
        "\n",
        "    return result_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R773PZlsv-6b"
      },
      "outputs": [],
      "source": [
        "## Retrieve text from webpages\n",
        "\n",
        "def get_full_text(url, word_limit=1000):\n",
        "    try:\n",
        "        page_response = requests.get(url)\n",
        "        if page_response.status_code != 200:\n",
        "            raise ConnectionError(f\"Failed to retrieve the page content with status code: {page_response.status_code}\")\n",
        "\n",
        "        soup = BeautifulSoup(page_response.content, 'html.parser')\n",
        "        paragraphs = soup.find_all('p')\n",
        "        full_text = ' '.join([p.get_text() for p in paragraphs])\n",
        "        words = full_text.split()\n",
        "        text = ' '.join(words[:word_limit]) if len(words) > word_limit else full_text\n",
        "\n",
        "        return text\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        raise ConnectionError(f\"Error retrieving full text from page: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z3MVIc5wCvq"
      },
      "outputs": [],
      "source": [
        "## Wikipedia Web Crawler\n",
        "\n",
        "def scrape_wiki(query,k=1,word_limit=1000):\n",
        "    base_url = \"https://en.wikipedia.org/wiki/\"\n",
        "    page_url = f\"{base_url}{urllib.parse.quote(query)}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(page_url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        paragraphs = soup.find_all('p', limit=5)\n",
        "        content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])\n",
        "        words = content.split()\n",
        "        limited_content = ' '.join(words[:word_limit]) if len(words) > word_limit else content\n",
        "\n",
        "        return [{\n",
        "            \"title\": query,\n",
        "            \"link\": page_url,\n",
        "            \"content\": limited_content\n",
        "        }]\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        # force wiki search with gsearch API\n",
        "        query = query + \" Wikipedia\"\n",
        "        result_wiki = google_search(query)\n",
        "        text = get_full_text(result_wiki[0]['link'])\n",
        "        return [{\n",
        "            \"title\": query,\n",
        "            \"link\": result_wiki[0]['link'],\n",
        "            \"content\": text\n",
        "        }]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BAeqSaKwE4u"
      },
      "outputs": [],
      "source": [
        "## Stack Exchange Web Crawler\n",
        "\n",
        "stackexchange_api_url = 'https://api.stackexchange.com/2.3'\n",
        "def scrape_stackexchange(query,k=1):\n",
        "    sites = ['privacy policies', 'law', 'legal laws','policies']\n",
        "    questions_data = []\n",
        "\n",
        "    for site in sites:\n",
        "        # Build API query\n",
        "        params = {\n",
        "              'site': site,\n",
        "              'tagged': 'privacy',\n",
        "              'q': '',\n",
        "              'sort': 'votes',\n",
        "              'order': 'desc'\n",
        "          }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(f\"{stackexchange_api_url}/search/advanced\", params=params)\n",
        "            if response.status_code == 200:\n",
        "                  for question in response.json()['items']:\n",
        "                      questions_data.append({\n",
        "                          'site': site,\n",
        "                          'title': question['title'],\n",
        "                          'link': question['link'],\n",
        "                          'score': question['score']\n",
        "                      })\n",
        "        except:\n",
        "            print(\"StackEx error\")\n",
        "\n",
        "    return questions_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYo_se4FwG6O"
      },
      "outputs": [],
      "source": [
        "## CFPB Web Crawler\n",
        "\n",
        "CFPB_api_url = \"https://www.consumerfinance.gov/data-research/consumer-complaints/search/api/v1/\"\n",
        "def scrape_CFPB(query,k=1):\n",
        "    params = {\n",
        "                'company': 'pathway',\n",
        "                'issue': 'policies',\n",
        "                'date_received_max': datetime.now().strftime('%Y-%m-%d')\n",
        "            }\n",
        "\n",
        "    complaints_data = []\n",
        "    response = requests.get(CFPB_api_url, params=params)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "            for complaint in response.json()['hits']['hits']:\n",
        "                complaint_data = complaint['_source']\n",
        "                complaints_data.append({\n",
        "                    'title': complaint_data.get('issue', ''),\n",
        "                    'content': complaint_data.get('state', ''),\n",
        "                })\n",
        "\n",
        "    return complaints_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xLHuYwvwIoX"
      },
      "outputs": [],
      "source": [
        "## Reddit Web Crawler\n",
        "\n",
        "def scrape_reddit(query, k=1):\n",
        "    relevant_subreddits = ['privacy','PrivacyGuides', 'legal']\n",
        "    url = \"https://socialgrep.p.rapidapi.com/search/posts\"\n",
        "\n",
        "    posts_data = []\n",
        "    for subreddit_name in relevant_subreddits:\n",
        "            query_form = f\"/r/{subreddit_name},{query}\"\n",
        "            querystring = {\"query\":query_form}\n",
        "\n",
        "            headers = {\n",
        "              \"x-rapidapi-key\": \"e578b1448dmshe1c37e86db71948p125563jsn1f53582a5fb1\",\n",
        "              \"x-rapidapi-host\": \"socialgrep.p.rapidapi.com\"\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, headers=headers, params=querystring)\n",
        "            try:\n",
        "                posts_data.append(response.json())\n",
        "            except Exception as e:\n",
        "                print(\"Reddit API failed with error:\", e)\n",
        "\n",
        "\n",
        "    return posts_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aaxgl1pnwKf-"
      },
      "outputs": [],
      "source": [
        "def search(query, k=1):\n",
        "    methods = [google_search, bing_search, scrape_wiki, scrape_CFPB]\n",
        "    current_method = 0\n",
        "    result = []\n",
        "    for _ in range(len(methods)):\n",
        "        try:\n",
        "            result.append(methods[current_method](query,k))\n",
        "            print(f\"Using {methods[current_method].__name__}\")\n",
        "        except ConnectionError as e:\n",
        "            print(e)\n",
        "            result.append({})\n",
        "        current_method = (current_method + 1) % len(methods)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Web content retrieval through different web crawlers\n",
        "\n",
        "def web_rag_retrieval(cot_list):\n",
        "    methods = [\"Google\", \"Bing\", \"Wikipedia\", \"ConsumerFinance\"]\n",
        "    idx = 0\n",
        "    result_dict = {'Google':[],'Bing':[],'Wikipedia':[],'StackExchange':[],'ConsumerFinance':[]}\n",
        "    query = cot_list[1]\n",
        "    stack_result = scrape_stackexchange(query,3)\n",
        "    for r in stack_result:\n",
        "        try:\n",
        "            if 'link' in r:\n",
        "                substring = r\"stack exchange network consists of 183 q&a communities including stack overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers. now available on stack overflow for teams! ai features where you work: search, ide, and chat.\\s+ask questions, find answers and collaborate at work with stack overflow for teams.\\s+explore teams teams q&a for work connect and share knowledge within a single location that is structured and easy to search.\"\n",
        "                text = get_full_text(r['link']).lower().replace('\\t','').replace('\\n','')\n",
        "                text = re.sub(substring, '', text)\n",
        "                result_dict['StackExchange'].append([r['title'],str(text)])\n",
        "        except:\n",
        "            print(\"Error\")\n",
        "            result_dict[\"StackExchange\"].append(['',''])\n",
        "\n",
        "    for clist in cot_list:\n",
        "        query = clist\n",
        "        result = search(query, 5)\n",
        "        for kth_result in result:\n",
        "            for r in kth_result:\n",
        "                try:\n",
        "                    if 'link' in r:\n",
        "                        text = get_full_text(r['link']).lower().replace('\\t','').replace('\\n','')\n",
        "                        #upload_to_doc(r['link'])\n",
        "                    else:\n",
        "                        text = r['content']\n",
        "                    if idx<2:\n",
        "                        print(methods[idx]);\n",
        "                        result_dict[methods[idx]].append([r['link'],str(text)])\n",
        "                    else:\n",
        "                        result_dict[methods[idx]].append([r['title'],str(text)])\n",
        "                except:\n",
        "                    print(\"Error\")\n",
        "                    result_dict[methods[idx]].append(['',''])\n",
        "\n",
        "            idx = (idx + 1) % len(methods)\n",
        "\n",
        "    return result_dict"
      ],
      "metadata": {
        "id": "yieTKjx8q38L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# aggregator, context relevancy and embeddings\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def encode_texts(texts: List[str], model_name: str = 'all-MiniLM-L6-v2') -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Encode texts into vector embeddings using sentence transformers\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(texts, convert_to_tensor=True)\n",
        "    return embeddings.cpu().numpy()\n",
        "\n",
        "def filter_by_sentiment(texts: List[str], sentiment_threshold: float = 0.1) -> List[int]:\n",
        "    \"\"\"\n",
        "    Filter texts based on sentiment score\n",
        "    Returns indices of texts with positive sentiment above the threshold\n",
        "    \"\"\"\n",
        "    sentiment_analyzer = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "    positive_indices = []\n",
        "    for idx, text in enumerate(texts):\n",
        "        text = text[:512]\n",
        "        result = sentiment_analyzer(text)\n",
        "        if result[0]['label'] == 'POSITIVE' and result[0]['score'] > sentiment_threshold:\n",
        "            positive_indices.append(idx)\n",
        "\n",
        "    return positive_indices\n",
        "\n",
        "def build_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatIP:\n",
        "    \"\"\"\n",
        "    Build and return a FAISS index from embeddings\n",
        "    \"\"\"\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dimension)\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "def prepare_search_data(results_dict: Dict[str, List[List[str]]], sentiment_threshold: float) -> Tuple[List[str], List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Prepare flattened lists of texts, URLs, and sources from the nested dictionary\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    urls = []\n",
        "    sources = []\n",
        "\n",
        "    # Flatten the structure while keeping track of URLs and sources\n",
        "    for source, url_content_pairs in results_dict.items():\n",
        "        for url, content in url_content_pairs:\n",
        "            texts.append(content)\n",
        "            urls.append(url)\n",
        "            sources.append(source)\n",
        "\n",
        "    # Get indices of positive sentiment texts\n",
        "    positive_indices = filter_by_sentiment(texts, sentiment_threshold)\n",
        "\n",
        "    # Filter all lists using the positive indices\n",
        "    filtered_texts = [texts[i] for i in positive_indices]\n",
        "    filtered_urls = [urls[i] for i in positive_indices]\n",
        "    filtered_sources = [sources[i] for i in positive_indices]\n",
        "\n",
        "    return filtered_texts, filtered_urls, filtered_sources\n",
        "\n",
        "def search_similar(query: str, index: faiss.IndexFlatIP, model_name: str, top_k: int) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Search for similar texts using the FAISS index\n",
        "    \"\"\"\n",
        "    query_vector = encode_texts([query], model_name)\n",
        "    faiss.normalize_L2(query_vector)\n",
        "    return index.search(query_vector, top_k)\n",
        "\n",
        "def format_results(scores: np.ndarray, indices: np.ndarray, texts: List[str], urls: List[str], sources: List[str]) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Format search results into a list of dictionaries including URLs\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], indices[0]):\n",
        "        results.append({\n",
        "            'source': sources[idx],\n",
        "            'url': urls[idx],\n",
        "            'content': texts[idx],\n",
        "            'similarity_score': float(score)\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def aggregate_and_rank_results_faiss(\n",
        "    query: str,\n",
        "    results_dict: Dict[str, List[List[str]]],\n",
        "    top_k: int = 5,\n",
        "    sentiment_threshold: float = 0.1,\n",
        "    model_name: str = 'all-MiniLM-L6-v2',\n",
        "    output_file: str = 'search_results.json'\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Main function to process query and results using FAISS with sentiment filtering\n",
        "\n",
        "    Args:\n",
        "        query: Search query\n",
        "        results_dict: Dictionary mapping sources to lists of [url, content] pairs\n",
        "        top_k: Number of top results to return\n",
        "        sentiment_threshold: Minimum sentiment score to keep a text (0 to 1)\n",
        "        model_name: Name of the sentence transformer model to use\n",
        "        output_file: Path to save JSON output\n",
        "    \"\"\"\n",
        "    # Prepare data with sentiment filtering\n",
        "    texts, urls, sources = prepare_search_data(results_dict, sentiment_threshold)\n",
        "\n",
        "    if not texts:\n",
        "        print(\"No texts passed the sentiment threshold!\")\n",
        "        return []\n",
        "\n",
        "    # Create embeddings and index\n",
        "    embeddings = encode_texts(texts, model_name)\n",
        "    index = build_faiss_index(embeddings)\n",
        "\n",
        "    # Perform search\n",
        "    scores, indices = search_similar(query, index, model_name, min(top_k, len(texts)))\n",
        "\n",
        "    # Format results\n",
        "    results = format_results(scores, indices, texts, urls, sources)\n",
        "\n",
        "    # Save results to JSON file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "X3nRydhMxsQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HoFFQ8uNxtZu"
      },
      "outputs": [],
      "source": [
        "def web_rag_retrieval(cot_list):\n",
        "    methods = [\"Google\", \"Bing\", \"Wikipedia\", \"ConsumerFinance\"]\n",
        "    idx = 0\n",
        "    result_dict = {'Google':[],'Bing':[],'Wikipedia':[],'StackExchange':[],'ConsumerFinance':[]}\n",
        "    query = cot_list[1]\n",
        "    stack_result = scrape_stackexchange(query,3)\n",
        "    for r in stack_result:\n",
        "        try:\n",
        "            print(\"Search Result:\", r)\n",
        "            if 'link' in r:\n",
        "                substring = r\"stack exchange network consists of 183 q&a communities including stack overflow, the largest, most trusted online community for developers to learn, share their knowledge, and build their careers. now available on stack overflow for teams! ai features where you work: search, ide, and chat.\\s+ask questions, find answers and collaborate at work with stack overflow for teams.\\s+explore teams teams q&a for work connect and share knowledge within a single location that is structured and easy to search.\"\n",
        "                text = get_full_text(r['link']).lower().replace('\\t','').replace('\\n','')\n",
        "                text = re.sub(substring, '', text)\n",
        "                #upload_to_doc(r['link'])\n",
        "                print(\"Text Result:\",text)\n",
        "                result_dict['StackExchange'].append([r['title'],str(text)])\n",
        "        except:\n",
        "            print(\"Error\")\n",
        "            result_dict[\"StackExchange\"].append(['',''])\n",
        "\n",
        "    for clist in cot_list:\n",
        "        query = clist\n",
        "        result = search(query, 5)\n",
        "        for kth_result in result:\n",
        "            for r in kth_result:\n",
        "                try:\n",
        "                    print(\"Search Result:\", r)\n",
        "                    if 'link' in r:\n",
        "                        text = get_full_text(r['link']).lower().replace('\\t','').replace('\\n','')\n",
        "                        #upload_to_doc(r['link'])\n",
        "                    else:\n",
        "                        text = r['content']\n",
        "                    if idx<2:\n",
        "                        print(\"Text Result:\", text)\n",
        "                        print(methods[idx]);\n",
        "                        result_dict[methods[idx]].append([r['link'],str(text)])\n",
        "                    else:\n",
        "                        print(\"Text Result:\",)\n",
        "                        result_dict[methods[idx]].append([r['title'],str(text)])\n",
        "                except:\n",
        "                    print(\"Error\")\n",
        "                    result_dict[methods[idx]].append(['',''])\n",
        "\n",
        "            idx = (idx + 1) % len(methods)\n",
        "\n",
        "    return result_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval Pipeline - Using Pathway"
      ],
      "metadata": {
        "id": "QePs6x8_s1MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathway as pw\n",
        "\n",
        "# the static data has to be present under the ./data directory\n",
        "data_sources = []\n",
        "data_sources.append(\n",
        "    pw.io.fs.read(\n",
        "        \"./data\",\n",
        "        format=\"binary\",\n",
        "        mode=\"streaming\",\n",
        "        with_metadata=True,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "Oojq8jHiQTN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathway.xpacks.llm.vector_store import VectorStoreServer\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "\n",
        "transformations_example = [\n",
        "    TokenTextSplitter(\n",
        "        chunk_size=400,\n",
        "        chunk_overlap=100,\n",
        "        separator=\" \",\n",
        "    ),\n",
        "    embed_model,\n",
        "]\n",
        "\n",
        "## Data Preprocessing Pipeline\n",
        "processing_pipeline = VectorStoreServer.from_llamaindex_components(\n",
        "    *data_sources,\n",
        "    transformations=transformations_example,\n",
        ")\n",
        "\n",
        "## Setting up Pathway VectorStore Server\n",
        "PATHWAY_HOST = \"127.0.0.1\"\n",
        "PATHWAY_PORT = 8754\n",
        "processing_pipeline.run_server(\n",
        "    host=PATHWAY_HOST, port=PATHWAY_PORT, with_cache=False, threaded=True\n",
        ")"
      ],
      "metadata": {
        "id": "X4cZVk0MQWMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.retrievers.pathway import PathwayRetriever\n",
        "\n",
        "# Initializing the Pathway Retriever which is hooked onto the Pathway VectorStore\n",
        "retriever = PathwayRetriever(host=PATHWAY_HOST, port=PATHWAY_PORT,)"
      ],
      "metadata": {
        "id": "DmEERBpQ02Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_query(query_input):\n",
        "  \"\"\"\n",
        "    Retrieves the relevant static data from the Pathway Vector Store.\n",
        "\n",
        "    Args:\n",
        "        query_input: string\n",
        "\n",
        "    Return:\n",
        "        text_content : string\n",
        "  \"\"\"\n",
        "  results = retriever.retrieve(str_or_query_bundle=query_input)\n",
        "  text_content = \"\"\n",
        "  for result in results:\n",
        "      text_content += result.node.text\n",
        "  return text_content"
      ],
      "metadata": {
        "id": "i-908OuPQsZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "rag_query(\"Does dallascollege store cookies?\")"
      ],
      "metadata": {
        "id": "yAvGuXuSQ4S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_and_dump_retrieved_text(query_input,filename=\"retrieved.txt\"):\n",
        "  \"\"\"\n",
        "    Retrieves the relevant static data from the Pathway Vector Store.\n",
        "\n",
        "    Args:\n",
        "        query_input: string\n",
        "\n",
        "    Return:\n",
        "       None\n",
        "\n",
        "  \"\"\"\n",
        "  try:\n",
        "    retrieved_data = rag_query(query_input)\n",
        "    with open(f\"/content/temp/{filename}\", 'w') as f:\n",
        "        f.write(retrieved_data)\n",
        "    f.close()\n",
        "  except Exception as e:\n",
        "    return e"
      ],
      "metadata": {
        "id": "ofQPodRZRF1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "get_and_dump_retrieved_text(\"What information does Yahoo store?\")"
      ],
      "metadata": {
        "id": "ISk0XAen1Hq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Knowledge Graph"
      ],
      "metadata": {
        "id": "Kg7tENvbxcHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "from llama_index.core import KnowledgeGraphIndex\n",
        "from llama_index.core.graph_stores import SimpleGraphStore\n",
        "from llama_index.core.storage.storage_context import StorageContext"
      ],
      "metadata": {
        "id": "-tzH4922Verh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Generates evidence graph based on the .txt files dumped onto the ./temp/ directory\n",
        "def generate_evidence_graph(filepath=\"/content/temp/\"):\n",
        "    documents = SimpleDirectoryReader(filepath).load_data()\n",
        "    graph_store = SimpleGraphStore()\n",
        "    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
        "    index = KnowledgeGraphIndex.from_documents(\n",
        "        documents=documents,\n",
        "        max_triplets_per_chunk=3,\n",
        "        storage_context=storage_context,\n",
        "        embed_model=embed_model,\n",
        "        include_embeddings=True\n",
        "    )\n",
        "    storage_context.persist()\n",
        "\n",
        "    return index\n"
      ],
      "metadata": {
        "id": "GNb-Wm2cx0Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Visualizes the graph network\n",
        "from pyvis.network import Network\n",
        "from IPython.display import display\n",
        "import IPython\n",
        "g = index.get_networkx_graph()\n",
        "net = Network(notebook=True,cdn_resources=\"in_line\",directed=True)\n",
        "net.from_nx(g)\n",
        "net.show(\"graph.html\")\n",
        "net.save_graph(\"Knowledge_graph.html\")\n",
        "IPython.display.HTML(filename=\"/content/Knowledge_graph.html\")"
      ],
      "metadata": {
        "id": "4ml_qfvBVG2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Retreives all relevant nodes based on user query and generates relevant context for final response generation\n",
        "def retrieve_nodes(query):\n",
        "  query_engine = index.as_query_engine(include_text=True,\n",
        "                                      response_mode =\"tree_summarize\",\n",
        "                                      embedding_mode=\"hybrid\",\n",
        "                                      similarity_top_k=5,)\n",
        "  response = query_engine.query(query)\n",
        "  return response"
      ],
      "metadata": {
        "id": "mAdsIDqaUrPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieve_nodes(\"What information does Yahoo Store?\")"
      ],
      "metadata": {
        "id": "XOv78P0aU8Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "AjjPMfqvXXY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining RAG+Evidence Graph"
      ],
      "metadata": {
        "id": "j9WuL2um0Erx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Combinining RAG+Evidence Graph Generator\n",
        "def generate_context(query):\n",
        "  try:\n",
        "    get_and_dump_retrieved_text(query)\n",
        "    time.sleep(0.5)\n",
        "    index = generate_evidence_graph()\n",
        "    response = retrieve_nodes(query)\n",
        "    return response\n",
        "  except Exception as e:\n",
        "    return e"
      ],
      "metadata": {
        "id": "J8DFuy4xzt_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "get_and_dump_retrieved_text(\"What information does Yahoo Store?\")\n",
        "index = generate_evidence_graph()\n",
        "g = index.get_networkx_graph()\n",
        "net = Network(notebook=True,cdn_resources=\"in_line\",directed=True)\n",
        "net.from_nx(g)\n",
        "net.show(\"graph.html\")\n",
        "net.save_graph(\"Knowledge_graph.html\")\n",
        "IPython.display.HTML(filename=\"/content/Knowledge_graph.html\")"
      ],
      "metadata": {
        "id": "QqhjLk570Dk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieve_nodes(\"What information does Yahoo Store?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "tIilD3c54QST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the online privacy policy guidelines at Bank of America?\"\n",
        "get_and_dump_retrieved_text(query)\n",
        "index = generate_evidence_graph()\n",
        "g = index.get_networkx_graph()\n",
        "net = Network(notebook=True,cdn_resources=\"in_line\",directed=True)\n",
        "net.from_nx(g)\n",
        "net.show(\"graph.html\")\n",
        "net.save_graph(\"Knowledge_graph.html\")\n",
        "IPython.display.HTML(filename=\"/content/Knowledge_graph.html\")"
      ],
      "metadata": {
        "id": "uo43sksgHkdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieve_nodes(query)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "8aO7SD4EHtCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Response Generation"
      ],
      "metadata": {
        "id": "NrwVqBWIxUpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import time"
      ],
      "metadata": {
        "id": "Mn1qN1q81iyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Main Function\n",
        "def llm_query_2(query):\n",
        "    start = time.time()\n",
        "    get_and_dump_retrieved_text(query)\n",
        "    cot_list = cot_tasks(None, query)\n",
        "    result_dict = web_rag_retrieval(cot_list)\n",
        "    results = aggregate_and_rank_results_faiss(\n",
        "        query=query,\n",
        "        results_dict=result_dict,\n",
        "        top_k=5,\n",
        "        sentiment_threshold=0.1\n",
        "    )\n",
        "\n",
        "    with open('/content/search_results.json', 'r') as file:\n",
        "        entry = json.load(file)\n",
        "\n",
        "    content_list_web_rag_retrieval = [\n",
        "        entry[i]['content'].strip().lower() for i in range(len(entry))\n",
        "    ]\n",
        "    content = ''.join(f\"Content: {entry}\\n\" for entry in content_list_web_rag_retrieval)\n",
        "\n",
        "    with open(\"/content/temp/search.txt\", \"w\") as f:\n",
        "        f.write(content)\n",
        "\n",
        "    index = generate_evidence_graph()\n",
        "    g = index.get_networkx_graph()\n",
        "    net = Network(notebook=True,cdn_resources=\"in_line\",directed=True)\n",
        "    net.from_nx(g)\n",
        "    net.show(\"graph.html\")\n",
        "    net.save_graph(\"Knowledge_graph.html\")\n",
        "    IPython.display.HTML(filename=\"/content/Knowledge_graph.html\")\n",
        "\n",
        "    time.sleep(1)\n",
        "    context = retrieve_nodes(query)\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Context information is below.\n",
        "    ---------------------\n",
        "    {context}\n",
        "    ---------------------\n",
        "    Given the context information and not prior knowledge, answer the query.\n",
        "    Query: {query}\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    final_result = mistral_response(prompt)\n",
        "\n",
        "    end = time.time()\n",
        "    time_taken = end - start\n",
        "\n",
        "    return final_result, time_taken"
      ],
      "metadata": {
        "id": "-Il-PFkXIHU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_result, time_taken = llm_query_2(\"What information does Yahoo store?\")"
      ],
      "metadata": {
        "id": "hBUyI2QK_soD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install codecov\n",
        "!pip install guardrails"
      ],
      "metadata": {
        "id": "ypYCGDiL5Mh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from guardrails.hub import LogicCheck\n",
        "from guardrails import Guard\n",
        "\n",
        "guard = Guard.use(\n",
        "    LogicCheck()\n",
        ")\n",
        "\n",
        "# Validate the LLM response using Guardrails\n",
        "validation_result = guard.validate(response)\n",
        "\n",
        "if validation_result.passed:\n",
        "    print(\"LLM response is valid:\", llm_response)\n",
        "else:\n",
        "    print(\"LLM response failed validation:\", llm_response)\n",
        "    print(\"Reason:\", validation_result.reason)"
      ],
      "metadata": {
        "id": "8mg4TnNy5B8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_result)\n",
        "print(f\"Time Taken:{time_taken}\")"
      ],
      "metadata": {
        "id": "TmXpbqclB0nn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}